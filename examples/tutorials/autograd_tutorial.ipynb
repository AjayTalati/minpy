{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd System in MinPy\n",
    "\n",
    "We design MinPy with an idea in mind: how to maximize the flexibly and the convenience of a tool built for machine learning at the same time? If we want the flexibly, we need imperative programing to let researchers control every details of the neural network. However, in most of the machine learning tools, it is inevitable to manually implement the back propagation of the forward pass for manually defined layers or modules. Only the symbolic part of the network enjoys its hand-free back propagation. To integrate the automatic gradient solver and imperative programming together, we introduce the key component of MinPy's imperative programming - Autograd system.\n",
    "\n",
    "## A Close Look at Autograd System\n",
    "Autograd computes a gradient function for any customized function with a single output. For example, we define a simple function `foo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x):\n",
    "    return x**2\n",
    "\n",
    "foo(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get its derivative by MinPy's Autograd. To use Autograd, simply import `grad` from `minpy.core`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import minpy.numpy as np  # currently need import this at the same time\n",
    "from minpy.core import grad\n",
    "\n",
    "d_foo = grad(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_foo(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd also differentiates vector inputs. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ 2.  4.  6.  8.]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "d_foo(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd for Loss Function\n",
    "\n",
    "Since in world of machine learning we optimize a scalar loss, Autograd is particular useful to obtain the gradient of input parameters for next updates. For example, we define an affine layer, relu layer, and a softmax loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    \"\"\"\n",
    "    out = np.dot(x, w) + b\n",
    "    return out\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    \"\"\"\n",
    "    out = np.maximum(0, x)\n",
    "    return out\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    \"\"\"\n",
    "    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arange(N), y] -= 1\n",
    "    dx /= N\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use these layers to define a single layer fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleNet(object):\n",
    "    def __init__(self, hidden_size=100, num_class=3):\n",
    "        # Define model parameters.\n",
    "        self.params = {}\n",
    "        self.params['w'] = np.random.randn(hidden_size, num_class) * 0.01\n",
    "        self.params['b'] = 0\n",
    "\n",
    "    def forward(self, X):\n",
    "        # First affine layer (fully-connected layer).\n",
    "        y1 = affine(X, self.params['w'], self.params['b'])\n",
    "        # ReLU activation.\n",
    "        y2 = relu(y1)\n",
    "        return y2\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        # Compute softmax loss between the output and the label.\n",
    "        return softmax_loss(self.forward(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "hidden_size = 100\n",
    "net = SimpleNet(hidden_size)\n",
    "x = np.random.randn(batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient = grad(net.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get gradient by simply call `gradient(X, y)`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
